{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code taken from : Tech-At-Work in this video [Easily Fine Tune ChatGPT 3.5 to Outperform GPT-4!](https://youtu.be/8Ieu2v0v4oc?si=LLlZB0arElYWJWVH) (accessed last 20.11.2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SVoU7VrzGzxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.27.10)\n",
      "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.25.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2023.5.5)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Downloading tiktoken-0.5.1-cp311-cp311-macosx_10_9_x86_64.whl (953 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.5/953.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install numpy\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WysuPFYQHfCq"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ofhSD5BII1W"
   },
   "outputs": [],
   "source": [
    "# please replace this string with the actual api key\n",
    "openai.api_key = \"API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ySonCJu2I1KD"
   },
   "outputs": [],
   "source": [
    "#Load CSV data in for train_50.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_50.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train50-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-NNdePOiKuKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 200\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train50-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 658\n",
      "mean / median: 272.82, 249.5\n",
      "p5 / p95: 173.8, 407.5\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 409\n",
      "mean / median: 104.32, 86.5\n",
      "p5 / p95: 27.9, 204.60000000000002\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~13641 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~40923 tokens\n",
      "Estimated cost for fine-tuning: approximately $0.33\n"
     ]
    }
   ],
   "source": [
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the dataset as a JSONL file\n",
    "def save_to_jsonl(conversations, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for conversation in conversations:\n",
    "            json_line = json.dumps(conversation)\n",
    "            file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GME1BGqXNGMQ"
   },
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train50-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d403QpVVN3BU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-hmZVvME5IHk7zJ7TlSXNXDFk\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train50-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SlZEZYGTOY-L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-LjWWFBZ2LVJMcErWp3BOVwoz\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1698946835,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-hmZVvME5IHk7zJ7TlSXNXDFk\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-50\"\n",
    "\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1Xyzue2SPYaZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-LjWWFBZ2LVJMcErWp3BOVwoz\n",
      "Validating training file: file-hmZVvME5IHk7zJ7TlSXNXDFk\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/150: training loss=0.62\n",
      "Step 11/150: training loss=0.09\n",
      "Step 21/150: training loss=0.06\n",
      "Step 31/150: training loss=0.13\n",
      "Step 41/150: training loss=0.05\n",
      "Step 51/150: training loss=0.01\n",
      "Step 61/150: training loss=0.08\n",
      "Step 71/150: training loss=0.01\n",
      "Step 81/150: training loss=0.21\n",
      "Step 91/150: training loss=0.00\n",
      "Step 101/150: training loss=0.00\n",
      "Step 111/150: training loss=0.03\n",
      "Step 121/150: training loss=0.01\n",
      "Step 131/150: training loss=0.06\n",
      "Step 141/150: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-50:8GW6NJ4E\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QUgVjlQxUMhw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-LjWWFBZ2LVJMcErWp3BOVwoz\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1698946835,\n",
      "  \"finished_at\": 1698947338,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-50:8GW6NJ4E\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-CuG5MK0AFXkZOkLnf93ATNWG\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-hmZVvME5IHk7zJ7TlSXNXDFk\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 40623,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-50:8GW6NJ4E\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_100.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_100.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train100-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 100\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 658\n",
      "mean / median: 278.06, 249.5\n",
      "p5 / p95: 178.7, 412.70000000000005\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 409\n",
      "mean / median: 108.27, 87.0\n",
      "p5 / p95: 33.5, 212.50000000000006\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~27806 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~83418 tokens\n",
      "Estimated cost for fine-tuning: approximately $0.67\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train100-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train100-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-0LNOT5KHjfF74FdNgo8Mr8CY\n",
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-cwJaUkv3rT6oI2vxwfSJXLr8\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1698947569,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-0LNOT5KHjfF74FdNgo8Mr8CY\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train100-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)\n",
    "\n",
    "####################\n",
    "\n",
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-100\"\n",
    "\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-cwJaUkv3rT6oI2vxwfSJXLr8\n",
      "Validating training file: file-0LNOT5KHjfF74FdNgo8Mr8CY\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/300: training loss=0.17\n",
      "Step 11/300: training loss=0.20\n",
      "Step 21/300: training loss=0.24\n",
      "Step 31/300: training loss=0.05\n",
      "Step 41/300: training loss=0.03\n",
      "Step 51/300: training loss=0.02\n",
      "Step 61/300: training loss=0.07\n",
      "Step 71/300: training loss=0.04\n",
      "Step 81/300: training loss=0.09\n",
      "Step 91/300: training loss=0.00\n",
      "Step 101/300: training loss=0.00\n",
      "Step 111/300: training loss=0.02\n",
      "Step 121/300: training loss=0.04\n",
      "Step 131/300: training loss=0.00\n",
      "Step 141/300: training loss=0.00\n",
      "Step 151/300: training loss=0.02\n",
      "Step 161/300: training loss=0.00\n",
      "Step 171/300: training loss=0.11\n",
      "Step 181/300: training loss=0.11\n",
      "Step 191/300: training loss=0.05\n",
      "Step 201/300: training loss=0.00\n",
      "Step 211/300: training loss=0.00\n",
      "Step 221/300: training loss=0.14\n",
      "Step 231/300: training loss=0.00\n",
      "Step 241/300: training loss=0.11\n",
      "Step 251/300: training loss=0.01\n",
      "Step 261/300: training loss=0.00\n",
      "Step 271/300: training loss=0.21\n",
      "Step 281/300: training loss=0.00\n",
      "Step 291/300: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-100:8GWMiGKN\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-cwJaUkv3rT6oI2vxwfSJXLr8\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1698947569,\n",
      "  \"finished_at\": 1698948351,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-100:8GWMiGKN\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-l8tSmDlLxhqFQ0fddSpzjy1A\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-0LNOT5KHjfF74FdNgo8Mr8CY\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 82818,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-100:8GWMiGKN\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_200¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_200.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_200.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train200-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 200\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 689\n",
      "mean / median: 276.25, 249.0\n",
      "p5 / p95: 180.8, 406.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 417\n",
      "mean / median: 106.905, 86.0\n",
      "p5 / p95: 34.0, 203.39999999999998\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~55250 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~165750 tokens\n",
      "Estimated cost for fine-tuning: approximately $1.33\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train200-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train200-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-p4gs91wol87czouFTwDaKRYy\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train200-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-bPm1nWW1CmDNh5ihe8PIQeYQ\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699126937,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-p4gs91wol87czouFTwDaKRYy\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-200\"\n",
    "\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 121/600: training loss=0.01\n",
      "Step 131/600: training loss=0.34\n",
      "Step 141/600: training loss=0.03\n",
      "Step 151/600: training loss=0.04\n",
      "Step 161/600: training loss=0.01\n",
      "Step 171/600: training loss=0.06\n",
      "Step 181/600: training loss=0.00\n",
      "Step 191/600: training loss=0.00\n",
      "Step 201/600: training loss=0.08\n",
      "Step 211/600: training loss=0.00\n",
      "Step 221/600: training loss=0.00\n",
      "Step 231/600: training loss=0.06\n",
      "Step 241/600: training loss=0.04\n",
      "Step 251/600: training loss=0.00\n",
      "Step 261/600: training loss=0.02\n",
      "Step 271/600: training loss=0.00\n",
      "Step 281/600: training loss=0.00\n",
      "Step 291/600: training loss=0.00\n",
      "Step 301/600: training loss=0.03\n",
      "Step 311/600: training loss=0.00\n",
      "Step 321/600: training loss=0.26\n",
      "Step 331/600: training loss=0.06\n",
      "Step 341/600: training loss=0.00\n",
      "Step 351/600: training loss=0.04\n",
      "Step 361/600: training loss=0.06\n",
      "Step 371/600: training loss=0.15\n",
      "Step 381/600: training loss=0.00\n",
      "Step 391/600: training loss=0.00\n",
      "Step 401/600: training loss=0.08\n",
      "Step 411/600: training loss=0.07\n",
      "Step 421/600: training loss=0.00\n",
      "Step 431/600: training loss=0.02\n",
      "Step 441/600: training loss=0.16\n",
      "Step 451/600: training loss=0.05\n",
      "Step 461/600: training loss=0.01\n",
      "Step 471/600: training loss=0.00\n",
      "Step 481/600: training loss=0.00\n",
      "Step 491/600: training loss=0.00\n",
      "Step 501/600: training loss=0.00\n",
      "Step 511/600: training loss=0.00\n",
      "Step 521/600: training loss=0.00\n",
      "Step 531/600: training loss=0.00\n",
      "Step 541/600: training loss=0.00\n",
      "Step 551/600: training loss=0.04\n",
      "Step 561/600: training loss=0.24\n",
      "Step 571/600: training loss=0.27\n",
      "Step 581/600: training loss=0.01\n",
      "Step 591/600: training loss=0.06\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-200:8HHAT5rX\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-bPm1nWW1CmDNh5ihe8PIQeYQ\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699126937,\n",
      "  \"finished_at\": 1699128260,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-200:8HHAT5rX\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-nvfWh0w0rLlL461KIJeEA9EK\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-p4gs91wol87czouFTwDaKRYy\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 164550,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-200:8HHAT5rX\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_500¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_500.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_500.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train500-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 500\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 853\n",
      "mean / median: 278.11, 248.0\n",
      "p5 / p95: 177.9, 401.20000000000005\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 553\n",
      "mean / median: 108.296, 86.0\n",
      "p5 / p95: 30.900000000000006, 203.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~139055 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~417165 tokens\n",
      "Estimated cost for fine-tuning: approximately $3.34\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train500-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train500-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-nN3js1LWWjl7YBTM5NXDNYOd\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train500-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "This fine-tune request has been rate-limited. Your organization has reached the maximum of 3 active requests (3 running, 0 pending) for the model 'gpt-3.5-turbo-0613'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Create Fine-Tuning Job\u001b[39;00m\n\u001b[1;32m      2\u001b[0m suffix_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain-500\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFineTuningJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_file_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m job_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/createable_api_resource.py:57\u001b[0m, in \u001b[0;36mCreateableAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m     48\u001b[0m ):\n\u001b[1;32m     49\u001b[0m     requestor, url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_create_requestor(\n\u001b[1;32m     50\u001b[0m         api_key,\n\u001b[1;32m     51\u001b[0m         api_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m         organization,\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 57\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m     62\u001b[0m         response,\n\u001b[1;32m     63\u001b[0m         api_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         plain_old_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mplain_old_data,\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: This fine-tune request has been rate-limited. Your organization has reached the maximum of 3 active requests (3 running, 0 pending) for the model 'gpt-3.5-turbo-0613'."
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-500\"\n",
    "\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def create_fine_tuning_job(training_file_id):\n",
    "    response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    "    )\n",
    "    return response[\"id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-UrrnRGBYYD3kKkhhoVzrgtdY\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-500\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "job_id = response\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-UrrnRGBYYD3kKkhhoVzrgtdY\n",
      "Validating training file: file-nN3js1LWWjl7YBTM5NXDNYOd\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1500: training loss=0.35\n",
      "Step 101/1500: training loss=0.04\n",
      "Step 201/1500: training loss=0.13\n",
      "Step 301/1500: training loss=0.16\n",
      "Step 401/1500: training loss=0.04\n",
      "Step 501/1500: training loss=0.00\n",
      "Step 601/1500: training loss=0.00\n",
      "Step 701/1500: training loss=0.00\n",
      "Step 801/1500: training loss=0.00\n",
      "Step 901/1500: training loss=0.13\n",
      "Step 1001/1500: training loss=0.00\n",
      "Step 1101/1500: training loss=0.00\n",
      "Step 1201/1500: training loss=0.00\n",
      "Step 1301/1500: training loss=0.00\n",
      "Step 1401/1500: training loss=0.45\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-500:8HSFR0js\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-UrrnRGBYYD3kKkhhoVzrgtdY\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699168059,\n",
      "  \"finished_at\": 1699170852,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-500:8HSFR0js\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-O5a8MgzVpvtnDOHR737EBupu\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-nN3js1LWWjl7YBTM5NXDNYOd\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 414165,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-500:8HSFR0js\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_1000¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_1000.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_1000.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train1000-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 1000\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 1112\n",
      "mean / median: 273.239, 245.0\n",
      "p5 / p95: 179.0, 392.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 752\n",
      "mean / median: 104.591, 83.0\n",
      "p5 / p95: 33.0, 195.20000000000005\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~273239 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~819717 tokens\n",
      "Estimated cost for fine-tuning: approximately $6.56\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train1000-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train1000-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-RuKGfyKAbOUNlSUbiu8Moom0\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train1000-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-ZFTuIn2nlY1BAIZ6rDpgKxtp\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "\n",
    "suffix_name = \"train-1000\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-ZFTuIn2nlY1BAIZ6rDpgKxtp\n",
      "Validating training file: file-RuKGfyKAbOUNlSUbiu8Moom0\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1500: training loss=0.27\n",
      "Step 101/1500: training loss=0.11\n",
      "Step 201/1500: training loss=0.04\n",
      "Step 301/1500: training loss=0.09\n",
      "Step 401/1500: training loss=0.00\n",
      "Step 501/1500: training loss=0.00\n",
      "Step 601/1500: training loss=0.02\n",
      "Step 701/1500: training loss=0.03\n",
      "Step 801/1500: training loss=0.01\n",
      "Step 901/1500: training loss=0.04\n",
      "Step 1001/1500: training loss=0.01\n",
      "Step 1101/1500: training loss=0.00\n",
      "Step 1201/1500: training loss=0.00\n",
      "Step 1301/1500: training loss=0.00\n",
      "Step 1401/1500: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-1000:8HUHOHgt\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-ZFTuIn2nlY1BAIZ6rDpgKxtp\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699175903,\n",
      "  \"finished_at\": 1699178661,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-1000:8HUHOHgt\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-BHUNWT1F2RLbn2KtATQUZAMW\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-RuKGfyKAbOUNlSUbiu8Moom0\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 813717,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-1000:8HUHOHgt\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_2000¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_2000.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_2000.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train2000-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 2000\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 1112\n",
      "mean / median: 276.684, 245.0\n",
      "p5 / p95: 178.0, 406.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 752\n",
      "mean / median: 107.282, 83.0\n",
      "p5 / p95: 31.0, 206.10000000000014\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~553368 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~1660104 tokens\n",
      "Estimated cost for fine-tuning: approximately $13.28\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train2000-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train2000-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-zSGfzNRwUqGmGzgcggScrxPL\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train2000-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-cbYY7toGBQ8qWoJITmPs5sKF\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-2000\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-cbYY7toGBQ8qWoJITmPs5sKF\n",
      "Validating training file: file-zSGfzNRwUqGmGzgcggScrxPL\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1500: training loss=0.26\n",
      "Step 101/1500: training loss=0.07\n",
      "Step 201/1500: training loss=0.05\n",
      "Step 301/1500: training loss=0.01\n",
      "Step 401/1500: training loss=0.00\n",
      "Step 501/1500: training loss=0.00\n",
      "Step 601/1500: training loss=0.02\n",
      "Step 701/1500: training loss=0.00\n",
      "Step 801/1500: training loss=0.04\n",
      "Step 901/1500: training loss=0.01\n",
      "Step 1001/1500: training loss=0.02\n",
      "Step 1101/1500: training loss=0.05\n",
      "Step 1201/1500: training loss=0.00\n",
      "Step 1301/1500: training loss=0.07\n",
      "Step 1401/1500: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-2000:8HXS81Fi\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-cbYY7toGBQ8qWoJITmPs5sKF\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699187639,\n",
      "  \"finished_at\": 1699190859,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-2000:8HXS81Fi\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-e8kzJ2Em4QvJLCSzwygZfE5k\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-zSGfzNRwUqGmGzgcggScrxPL\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 1648104,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-2000:8HXS81Fi\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_5000¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_5000.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_5000.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train5000-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 5000\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 1545\n",
      "mean / median: 275.6108, 243.0\n",
      "p5 / p95: 179.0, 400.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 1067\n",
      "mean / median: 106.4704, 82.0\n",
      "p5 / p95: 32.0, 201.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~1378054 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~4134162 tokens\n",
      "Estimated cost for fine-tuning: approximately $33.07\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train5000-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train5000-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-Yq46yBwRn2R4szfpsrUyvHkT\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train5000-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-7DZF4DkExn5QmRd0ohFyEAS0\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-5000\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-7DZF4DkExn5QmRd0ohFyEAS0\n",
      "Validating training file: file-Yq46yBwRn2R4szfpsrUyvHkT\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1500: training loss=0.27\n",
      "Step 101/1500: training loss=0.03\n",
      "Step 201/1500: training loss=0.03\n",
      "Step 301/1500: training loss=0.02\n",
      "Step 401/1500: training loss=0.01\n",
      "Step 501/1500: training loss=0.02\n",
      "Step 601/1500: training loss=0.01\n",
      "Step 701/1500: training loss=0.01\n",
      "Step 801/1500: training loss=0.01\n",
      "Step 901/1500: training loss=0.02\n",
      "Step 1001/1500: training loss=0.00\n",
      "Step 1101/1500: training loss=0.02\n",
      "Step 1201/1500: training loss=0.00\n",
      "Step 1301/1500: training loss=0.01\n",
      "Step 1401/1500: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-5000:8HqJwp87\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-7DZF4DkExn5QmRd0ohFyEAS0\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699260899,\n",
      "  \"finished_at\": 1699263387,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-5000:8HqJwp87\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-mRqxe4ZJuINr88FQjMbBob8z\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-Yq46yBwRn2R4szfpsrUyvHkT\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 4104162,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-5000:8HqJwp87\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_10000¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_10000.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_10000.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train10000-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 10000\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 2140\n",
      "mean / median: 276.9378, 243.0\n",
      "p5 / p95: 179.0, 403.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 1547\n",
      "mean / median: 107.4908, 82.0\n",
      "p5 / p95: 32.0, 205.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~2769378 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~8308134 tokens\n",
      "Estimated cost for fine-tuning: approximately $66.47\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train10000-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train10000-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-YE4ZzTzKMNXqZYiT3lH1HA5D\n"
     ]
    }
   ],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train10000-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-ErsMkmBfoVtMAzMcFQTj3M0k\n"
     ]
    }
   ],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-10000\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-ErsMkmBfoVtMAzMcFQTj3M0k\n",
      "Validating training file: file-YE4ZzTzKMNXqZYiT3lH1HA5D\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1539: training loss=0.22\n",
      "Step 101/1539: training loss=0.02\n",
      "Step 201/1539: training loss=0.01\n",
      "Step 301/1539: training loss=0.01\n",
      "Step 401/1539: training loss=0.02\n",
      "Step 501/1539: training loss=0.01\n",
      "Step 601/1539: training loss=0.03\n",
      "Step 701/1539: training loss=0.01\n",
      "Step 801/1539: training loss=0.02\n",
      "Step 901/1539: training loss=0.01\n",
      "Step 1001/1539: training loss=0.02\n",
      "Step 1101/1539: training loss=0.01\n",
      "Step 1201/1539: training loss=0.00\n",
      "Step 1301/1539: training loss=0.01\n",
      "Step 1401/1539: training loss=0.01\n",
      "Step 1501/1539: training loss=0.01\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:cl-uzh:train-10000:8HrmXwL4\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-ErsMkmBfoVtMAzMcFQTj3M0k\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699264870,\n",
      "  \"finished_at\": 1699269004,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:cl-uzh:train-10000:8HrmXwL4\",\n",
      "  \"organization_id\": \"org-HjWBseLU0iDzg4cx01nO6JrY\",\n",
      "  \"result_files\": [\n",
      "    \"file-3PePRXGS3RUZGbxWHf6srrS8\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-YE4ZzTzKMNXqZYiT3lH1HA5D\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 2\n",
      "  },\n",
      "  \"trained_tokens\": 5498756,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:cl-uzh:train-10000:8HrmXwL4\n"
     ]
    }
   ],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_20000¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV data in for train_20000.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_20000.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train20000-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 20000\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 2241\n",
      "mean / median: 275.7084, 243.0\n",
      "p5 / p95: 179.0, 401.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 1621\n",
      "mean / median: 106.55645, 82.0\n",
      "p5 / p95: 32.0, 203.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~5514168 tokens that will be charged for during training\n",
      "By default, you'll train for 2 epochs on this dataset\n",
      "By default, you'll be charged for ~11028336 tokens\n",
      "Estimated cost for fine-tuning: approximately $88.23\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train20000-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train20000-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train20000-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-20000\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for model train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error for cell '{\"messages\":[{\"role\":\"system\",\"content\":\"You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.\"},{\"role\":\"user\",\"content\":\"Return the UPOS tag for the tokens of the sentence: ' Qui postquam tecta tyranni intravere sui , \" non est hortamine longo nunc \" ait \" utendum . . 'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"},{\"role\":\"assistant\",\"content\":\"Qui\\\\tPRON\\\\n postquam\\\\tCONJ\\\\n tecta\\\\tNOUN\\\\n tyranni\\\\tNOUN\\\\n intravere\\\\tVERB\\\\n sui\\\\tADJ\\\\n ,\\\\tPUNCT\\\\n \"\\\\tPUNCT\\\\n non\\\\tADV\\\\n est\\\\tVERB\\\\n hortamine\\\\tNOUN\\\\n longo\\\\tADJ\\\\n nunc\\\\tADV\\\\n \"\\\\tPUNCT\\\\n ait\\\\tVERB\\\\n \"\\\\tPUNCT\\\\n utendum\\\\tVERB\\\\n .\\\\tPUNCT\\\\n .\\\\tPUNCT\\\\n\"}]}': Expecting ',' delimiter: line 1 column 399 (char 398)\n"
     ]
    }
   ],
   "source": [
    "#Load CSV data in for train_total.csv\n",
    "csv_file_path = '../random-training-data-other-taggers/csv/train_total.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        for cell in row:\n",
    "            try:\n",
    "                # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'train_total-cleaned.jsonl'\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 43866\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Latin linguist and part-of-speech tagging expert. You are using UPOS (universal part of speech tags). UPOS tags are ADJ,ADP,ADV,AUX,CCONJ,DET,INTJ,NOUN,NUM,PART,PRON,PROPN,PUNCT,SCONJ,SYM,VERB and X. X stands for other.'}\n",
      "{'role': 'user', 'content': \"Return the UPOS tag for the tokens of the sentence: '  'Return the tags in the format TOKEN\\tTag\\n. Only output the token and the tag (no explanations, no translations, no additional text).\"}\n",
      "{'role': 'assistant', 'content': '\\\\t\\\\n'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 140, 2846\n",
      "mean / median: 276.54060092098666, 243.0\n",
      "p5 / p95: 179.0, 403.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 2089\n",
      "mean / median: 107.19965349017463, 82.0\n",
      "p5 / p95: 32.0, 205.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~12130730 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~12130730 tokens\n",
      "Estimated cost for fine-tuning: approximately $97.05\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'train_total-cleaned.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 50000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_1k_tokens = 0.0080  # Cost for every 1000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSONL file\n",
    "jsonl_file_path = 'train_total-cleaned.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload data for training\n",
    "training_file_name = 'train_total-cleaned.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "#Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fine-Tuning Job\n",
    "suffix_name = \"train-total\"\n",
    "\n",
    "response = create_fine_tuning_job(training_file_id)\n",
    "\n",
    "job_id = response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6lQoUlvASP9CU9mF3FbtT",
   "mount_file_id": "1OS7xqlAT9V4b_1eDO3XfFuRHKX5Vr3XX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
